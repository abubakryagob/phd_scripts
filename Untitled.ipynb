{
 "cells": [
  {
   "cell_type": "raw",
   "id": "32c25021",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the URL for the NuSTAR archive API\n",
    "url = 'https://heasarc.gsfc.nasa.gov/cgi-bin/vo/cone/coneGet.pl?table=nustar&'\n",
    "\n",
    "# Set the search parameters for magnetars\n",
    "params = {\n",
    "    'WHERE': \"(object like '1E%') OR (object like 'PSR%') OR (object like 'SGR%')\",\n",
    "    'VERB': 4,\n",
    "    'FORMAT': 'json'\n",
    "}\n",
    "\n",
    "# Send the request to the API and get the results\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the JSON response and extract the observation IDs\n",
    "try:\n",
    "    results = json.loads(response.text)['results']\n",
    "    observation_ids = [result['obsid'] for result in results]\n",
    "    print(observation_ids)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON response: {e}\")\n",
    "    print(response.text)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0155d489",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "# Set the URL for the NuSTAR archive API\n",
    "url = 'https://heasarc.gsfc.nasa.gov/cgi-bin/vo/cone/coneGet.pl'\n",
    "\n",
    "# Set the parameters for the query\n",
    "params = {\n",
    "    'tablehead': 'false',\n",
    "    'format': 'json',\n",
    "    'radius': '5',\n",
    "    'DEC': '>-80',\n",
    "    'fields': 'obsid,targetname,ra,dec,start_time,stop_time',\n",
    "    'mission': 'nustar',\n",
    "    'resultmax': '10000',\n",
    "    'object': 'magnetar'\n",
    "}\n",
    "\n",
    "# Send the GET request and store the response\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the response is valid JSON\n",
    "try:\n",
    "    results = response.json()['results']\n",
    "except ValueError:\n",
    "    # If the response is not valid JSON, print the error message and exit\n",
    "    print('Error decoding JSON response:', response.text)\n",
    "    exit()\n",
    "\n",
    "# Extract the observation IDs from the results\n",
    "observation_ids = [result['obsid'] for result in results]\n",
    "\n",
    "# Print the observation IDs for magnetars\n",
    "print(observation_ids)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a57e8bb",
   "metadata": {},
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define the base URL for the NuSTAR Data Archive API\n",
    "base_url = 'https://heasarc.gsfc.nasa.gov/cgi-bin/vo/cone/coneGet.pl?'\n",
    "\n",
    "# Define the coordinates and search radius for magnetars\n",
    "ra = '260.058'\n",
    "dec = '33.784'\n",
    "sr = '0.5'\n",
    "\n",
    "# Define the query parameters\n",
    "params = {\n",
    "    'tablehead': 'false',\n",
    "    'colRA': 'ra',\n",
    "    'colDec': 'dec',\n",
    "    'radius': sr,\n",
    "    'resultmax': 'unlimited',\n",
    "    'table': 'nustar',\n",
    "    'where': f'CONTAINS(POINT(\\'ICRS\\',nustar.ra,nustar.dec),CIRCLE(\\'ICRS\\',{ra},{dec},{sr})) AND nustar.instrume LIKE \\'FPMA\\' AND (nustar.src_names LIKE \\'%1E%\\')'\n",
    "}\n",
    "\n",
    "# Send the request to the NuSTAR Data Archive API\n",
    "response = requests.get(base_url, params=params)\n",
    "\n",
    "# Parse the VOTABLE response and extract the observation IDs\n",
    "root = ET.fromstring(response.text)\n",
    "resource = root.find('{http://www.ivoa.net/xml/VOTable/v1.3}RESOURCE')\n",
    "if resource is not None and resource.attrib['type'] == 'results':\n",
    "    results = resource.findall('{http://www.ivoa.net/xml/VOTable/v1.3}TABLEDATA/{http://www.ivoa.net/xml/VOTable/v1.3}TR')\n",
    "    observation_ids = [result.find('{http://www.ivoa.net/xml/VOTable/v1.3}TD[1]').text for result in results]\n",
    "else:\n",
    "    print('Error decoding VOTABLE response: ' + response.text)\n",
    "    exit()\n",
    "\n",
    "# Print the observation IDs for magnetars\n",
    "print(observation_ids)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b16f7c4f",
   "metadata": {},
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# URL of the directory containing the observation directories\n",
    "url = 'https://heasarc.gsfc.nasa.gov/FTP/nustar/data/obs/'\n",
    "\n",
    "# Function to recursively list all the observation IDs in each subdirectory\n",
    "def list_observation_ids(url, output_file):\n",
    "    response = requests.get(url)\n",
    "    lines = response.text.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith('d'):\n",
    "            subdir_url = url + line.split()\n",
    "            list_observation_ids(subdir_url, output_file)\n",
    "        else:\n",
    "            obsid = line.split()[-1]\n",
    "            if obsid.endswith('/'):\n",
    "                obsid = obsid[:-1]\n",
    "            output_file.write(obsid + '\\n')\n",
    "\n",
    "# Create the output file and start listing observation IDs\n",
    "with open('observation_ids.txt', 'w') as output_file:\n",
    "    list_observation_ids(url, output_file)\n",
    "    \n",
    "print('Observation IDs saved in observation_ids.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ce4012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /usr/local/anaconda3/lib/python3.8/site-packages (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from BeautifulSoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c67bb3d",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and directories to search\n",
    "base_url = \"https://heasarc.gsfc.nasa.gov/FTP/nustar/data/obs/\"\n",
    "dirs = [\"01\", \"02\"]\n",
    "\n",
    "def get_obs_ids(url, output_file):\n",
    "    # Get the HTML of the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the links in the HTML\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Loop through each link\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        # If the link ends with \"/\", it's a subdirectory\n",
    "        if href.endswith(\"/\"):\n",
    "            # Recursively call this function to process the subdirectory\n",
    "            subdir_url = url + href\n",
    "            get_obs_ids(subdir_url, output_file)\n",
    "        else:\n",
    "            # If the link ends with \".gz\", it's an observation ID\n",
    "            if href.endswith(\".gz\"):\n",
    "                obs_id = href[:-3] # Remove the \".gz\" extension\n",
    "                output_file.write(obs_id + \"\\n\") # Write the observation ID to the output file\n",
    "\n",
    "# Open the output file\n",
    "with open(\"observation_ids.txt\", \"w\") as output_file:\n",
    "    # Loop through each directory\n",
    "    for directory in dirs:\n",
    "        directory_url = base_url + directory + \"/\"\n",
    "        # Loop through each subdirectory\n",
    "        for subdirectory in range(10):\n",
    "            subdirectory_url = directory_url + str(subdirectory) + \"/\"\n",
    "            # Call the function to get the observation IDs for this subdirectory\n",
    "            get_obs_ids(subdirectory_url, output_file)\n",
    "\n",
    "# Print a message indicating that the observation IDs have been saved\n",
    "print(\"Observation IDs saved in observation_ids.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "084f9b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m subdirectory \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     32\u001b[0m             url \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m subdirectory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 33\u001b[0m             \u001b[43mlist_observation_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation IDs saved in observation_ids.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mlist_observation_ids\u001b[0;34m(url, output_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m     list_observation_ids(subdir_url, output_file)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Otherwise, the line is an observation ID\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the base URL for the NuSTAR observations\n",
    "base_url = 'https://heasarc.gsfc.nasa.gov/FTP/nustar/data/obs/'\n",
    "\n",
    "# Define a function to recursively list observation IDs\n",
    "def list_observation_ids(url, output_file):\n",
    "    \"\"\"\n",
    "    Recursively list observation IDs in the given URL and write them to the\n",
    "    specified output file.\n",
    "    \"\"\"\n",
    "    # Get the contents of the URL\n",
    "    contents = os.popen('curl -s ' + url).read()\n",
    "\n",
    "    # Split the contents into lines\n",
    "    lines = contents.split('\\n')\n",
    "\n",
    "    # Loop over the lines\n",
    "    for line in lines:\n",
    "        # If the line is a subdirectory, recurse into it\n",
    "        if line.endswith('/'):\n",
    "            subdir_url = url + line\n",
    "            list_observation_ids(subdir_url, output_file)\n",
    "        # Otherwise, the line is an observation ID\n",
    "        else:\n",
    "            output_file.write(line.split()[1] + '\\n')\n",
    "\n",
    "# Create the output file and start listing observation IDs\n",
    "with open('observation_ids.txt', 'w') as output_file:\n",
    "    for directory in ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09']:\n",
    "        for subdirectory in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "            url = base_url + directory + '/' + subdirectory + '/'\n",
    "            list_observation_ids(url, output_file)\n",
    "\n",
    "print('Observation IDs saved in observation_ids.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738b01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
